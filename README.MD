The methods of a random forest date back to 1995 in Ho et al.’s seminal work ‘Random Decision Forest’. It is a process that utilizes stochastic modeling, a technique that we have already seen in the class– recall that stochastic gradient descent is a process by which model parameters are found to be the best fit between predicted and target outputs. Also recall that ‘stochastic’ means random and in Ho et al.’s paper, we see the use of ‘randomly selected subspaces of a feature space’ to build multiple trees. The authors further show that trees within a subspace can complement other trees so as to improve overall classification.
Ho et al.’s work attempts to solve the general problems of tree creation/machine learning: overfitting and underfitting. For the authors, there are two methods for growing a tree: central axis projection and perceptron training, and yet each have their respective limitations which is stated above. For central axis projection, ‘With regard to overfitting, a tree can grow quite large and overly complex in the case of classification given a particular feature(1-2). It should be noted that the first way in which one can build classes relies on an error function and points above and below a hyperplane. The error function is computed by the following assumption: ‘assume that we have training points of two or more classes at any nonterminal node including the root. We first find the two classes whose means are furthest apart by Euclidean distance[...]the hyperplane that minimizes the sum of these counts is chosen for that node’(2). The drawback of this form of tree growth is that although it always stops when all leaves have a class, it is a ‘crude optimization that often leads to a very large tree’(2).
In the second method for a single classification tree, a perceptron is used which uses a decision hyperplane at each non-leaf node. Minimizing Euclidean distance is again that which determines the separation sets of data; however, there is no convergence point, since there is
no test on whether the data itself is linearly separable(2). That means a fixed number of iterations must be used in order to terminate this tree formation. This method produces a smaller tree, but training is ‘more expensive’(3).
After providing their readers with the main obstacles of the two types of decision trees, the authors present their solution: better accuracy with more trees. In running their experiments with random feature subspaces and different numbers of trees, the authors find a direct relationship between number of trees and accuracy.
In our results we come to the same conclusion. Overall, as the number of trees increases, so too does the accuracy of the nidek. However there is one major caveat: time. More trees increases the time it takes to train the model. Furthermore, as the authors found in their own results, the model reaches a limit where the number of trees no longer improves the accuracy of the model.



To Run File:

Install Jupyter Notebook: If you haven't installed Jupyter Notebook on your computer, you can do so by following the instructions on their website: https://jupyter.org/install
Open Jupyter Notebook: Once you have installed Jupyter Notebook, open it by typing jupyter notebook in your terminal or command prompt.
Navigate to the directory containing the .ipynb file: In Jupyter Notebook, navigate to the directory where the .ipynb file is located.
Open the .ipynb file: Click on the .ipynb file to open it in Jupyter Notebook.
Run the code: You can run the code in the .ipynb file by clicking on the "Run" button or by pressing "Shift + Enter" on your keyboard. You can also run all the code in the notebook by clicking on "Cell" in the menu bar, then "Run All."
